{"meta":{"title":"Wayne's blog","subtitle":"一个有情调的程序员","description":null,"author":"侯骅十","url":"http://yoursite.com","root":"/"},"pages":[{"title":"categories","date":"2019-06-05T08:43:39.000Z","updated":"2019-06-05T08:43:56.155Z","comments":false,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2019-06-05T08:42:46.000Z","updated":"2019-06-05T08:43:21.820Z","comments":false,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Redis应用实践——scan指令","slug":"Redis应用实践——scan指令","date":"2019-10-03T04:43:44.000Z","updated":"2019-10-03T04:48:11.824Z","comments":true,"path":"posts/17414/","link":"","permalink":"http://yoursite.com/posts/17414/","excerpt":"","text":"Redis应用实践——scan指令从某种程度上来说，scan是来取代keys指令的。传统的keys指令不仅没有limit这类的参数，而且是通过遍历查找，复杂度是O(n)。 scan的基本用法 scan 0 match key99** count 1000 上面的 0 为cursor游标参数，这条指令执行完，会返回一个游标， 返回的游标不为0则代表还没有遍历完成。 上面的 1000 为limit值，但是这个值并不是指定了返回结果集的数量，而是服务器本次遍历的字典槽位数量（约等于）。所以你会发现，设置了1000的limit，返回的结果可能只有10条左右。 scan的遍历顺序在你使用scan指令做了实验后，你可能发现，返回的游标并不是一次比一次大的。因为scan指令确实使用的不是从0开始顺序遍历的方法。 这是因为redis考虑到了字典扩容或者缩容的情况，所以使用了高位加法的方法来决定遍历的顺序。（原理代论证，这里不是很清楚）这样扩容后，可以继续从当前的槽往后遍历，之前的全部是已遍历过的，之后的都没有遍历过。 渐进式rehash在扩容的过程中，如果key比较大，那么对所有元素进行rehash将会消耗比较长的时间，而redis又是单线程的，所以采用了渐进式的策略，也就是同时维护两个数组，在查找元素时如果一个找不到就要区另一个里面找。scan命令也需要同时扫描新旧的槽位。 大key扫描如果一个key过大，在迁移时就会造成卡顿，而且在需要扩容时，也会一次性申请两倍大小的空间。 所以在业务开发中，尽量避免大key的产生，如果监控发现redis的内存大起大落，那就很可能时大key导致的问题。 redis-cli给我们提供了扫描大key的功能redis-cli -h 127.0.0.1 -p 7001 --bigkeys","categories":[],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"}]},{"title":"Redis应用实践——GeoHash","slug":"Redis应用实践——GeoHash","date":"2019-10-03T04:43:40.000Z","updated":"2019-10-03T04:48:11.824Z","comments":true,"path":"posts/19085/","link":"","permalink":"http://yoursite.com/posts/19085/","excerpt":"","text":"Redis应用实践——GeoHash业界比较通用的距离排序算法，是GeoHash，Redis也使用了GeoHash算法。 可以用来实现附近的人，附近的单车，附近的司机等等功能。 实现原理主要思路是将二维的坐标映射为一维的整数，比较基础容易理解的如二刀法。把正方形切一刀，然后再二分切一刀，再切 越来越小，然后每个切出来的正方形都用00,01,02这样的数字来表示，切的越多，数字越长，位置也更精确。 Geo算法会继续对这个整数做一次base32编码。在Redis中，将这个编码作为value放入zset中，score使用 经纬度编成的52位编码，可以用来排序距离。 基本用法 geoadd``geodist``getpos gethash georadiusbymember获取一定距离范围内的元素 georadius提供坐标查询一定距离范围内的元素 注意事项数据量很大之后，Geo的占用空间将会很大，如果单个key的数据过大，在迁移时会产生较大的性能影响。所以建议Geo使用单独的Redis实例部署。如果数据量更大，需要对Geo进行拆分，如按省市区来拆分。","categories":[],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"}]},{"title":"Redis应用实践——简单限流与漏斗限流","slug":"Redis应用实践——简单限流与漏斗限流","date":"2019-10-03T04:43:35.000Z","updated":"2019-10-03T04:48:11.826Z","comments":true,"path":"posts/50064/","link":"","permalink":"http://yoursite.com/posts/50064/","excerpt":"","text":"Redis应用实践——简单限流与漏斗限流简单限流实现原理就是使用zset结构，加上滑动窗口的思路来实现简单的时间窗口滑动。 zset的score存储时间戳。 代码中使用zremrangebyscore把时间窗口之前的访问记录都砍掉。 给每个新的记录设置一个过期时间（时间窗口的长度 可以再+1s） 漏斗限流如果自己实现一个漏斗算法需要如下 定义容量、漏水速率、漏水时间、剩余容量等 每次请求的时候用当前时间乘漏斗速率，看看需要的空间能不能满足需要的空间。 redis中提供的漏斗限流Redis4.0中开始提供了一个限流模块：REdis-Cell，该模块提供了限流算法，并且提供了原子命令。 该模块只有一条指令cl.throttle 使用方法cl.throttle key 15 40 60 1 15 ：漏斗容量 40 60: 40 operations /60 seconds 漏斗速率 1：可选参数 quota","categories":[],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"}]},{"title":"Redis应用实践——布隆过滤器","slug":"Redis应用实践——布隆过滤器","date":"2019-10-03T04:43:30.000Z","updated":"2019-10-03T04:48:11.826Z","comments":true,"path":"posts/3209/","link":"","permalink":"http://yoursite.com/posts/3209/","excerpt":"","text":"Redis应用⌚️——过滤器与限流布隆过滤器比如新闻客户端，在推送新的新闻时，要过滤掉已经看过这条新闻的用户。那用什么方法来过滤呢，你可能会想到缓存一个集合，但是当用户量很大的时候，所有用户的阅读历史集合将是一个非常庞大的数据。 如果你们没有为用户提供“足迹”这样的需求的话，上面所说的功能就可以利用布隆过滤器来实现。 什么是布隆过滤器布隆过滤器用来判断一个值是否不存在，但是不能完全准确的判断这个值是否一定存在，也就是会有误差，但是我们可以通过参数控制这个误差的概率。可以理解为一个不怎么精确的set结构，过滤时相当于contains方法。 在Java中，Guava里面提供了布隆过滤器的实现。 Redis中也提供了布隆过滤器的实现，在Redis 4.0后提供了布隆过滤器作为插件加入到Redis Server中。 使用方法使用bf.add bf.exists指令进行基础操作。 使用bf.madd``bf.mexists进行批量操作。 自定义参数的布隆过滤器使用bf.reserve有三个参数key error_rate initial_size。 error_rate越低，所需的空间越大。 initial_size注意，要尽量设置的准确，一旦元素数量超过initial_size，误判率将会大幅度上升。 布隆过滤器原理底层是使用一个大的数组和几个无偏hash函数来实现的，所谓无偏就是可能把hash值算得比较均匀。 当执行add时，使用几个hash函数分别算出来结果，每个结果对应一个位置，然后将数组这几个位置上的值都设成1。 当执行exists时，同样将传入的value通过上面的hash函数进行计算，在结果对应的几个位置上判断，如果各位上都是1，即证明这个值存在。 注意，证明值存在是会有误判的，因为存在可能 两个不同的value，通过hash计算出来的结果是完全一致的。 所以只能保证value一定不存在，不能完全保证这个value存在。 当这个数组越大的时候，存储也越稀疏，判断的准确性就会更高，但是占用的空间也会更大。","categories":[],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"}]},{"title":"Redis应用实践—HyperLogLog","slug":"Redis应用实践—HyperLogLog","date":"2019-10-03T04:43:13.000Z","updated":"2019-10-03T04:48:11.823Z","comments":true,"path":"posts/40227/","link":"","permalink":"http://yoursite.com/posts/40227/","excerpt":"","text":"Redis应用实践—HyperLogLog简介适用于精确度要求不是很高的统计需求。 比如你的网站上线运营了，老板想看看某个页面的uv有多少，你怎么办？ 给每个页面分配一个zset集合，当一个新用户访问到了，我们把用户id塞进集合就可以了，最后用scard统计集合的数量就ok了。 这确实是一个比较简单可行的方案，但是如果你们的网站成了爆款，火爆的页面可能有几千万访问，怎么办？买内存条？ 兄弟，大可不必.. Redis 为我们提供了一个数据结构就是为了给我们实现这样的功能的。 HyperLogLog在统计这样的数据时非常好用，因为它只占用非常少的空间。 使用方法 pfadd key userid0 pfcount key 还可以使用pfmerge合并多个结果 真的是太贴心了 拓展阅读：指令为什么用pf开头？是因为HyperLogLog这个数据结构的发明人是Philippe Flajolet ，pf就是他名字的字母缩写。 Redis对HyperLogLog的存储进行了优化，在开始是，使用稀疏矩阵存储，空间占用很小，只有当超过了一定的阈值之后才会转变成稠密矩阵来存储，这时它将占用12kb的内存空间。","categories":[],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"}]},{"title":"Redis应用实践——位图","slug":"Redis应用实践——位图","date":"2019-10-03T04:43:10.000Z","updated":"2019-10-03T04:48:11.825Z","comments":true,"path":"posts/57747/","link":"","permalink":"http://yoursite.com/posts/57747/","excerpt":"","text":"Redis应用实践——位图基础使用因为只能存取0或1，适用于大量的bool类型数据，比如一年的签到记录，如果使用普通的key/value存储那么用户量上来之后的存储空间时惊人的。 如果使用位图存储，365天只需要365bit ，只需要46个字节就可以完全存储。 使用setbit命令 setbit key 0 1 使用getbit命令获取某一位上的值 支持零存整取或者整存零取，零存即一个一个位进行set，整存就是直接set字符串。 使用bitcount进行统计 使用bitpos进行查找 批量操作在Redis3.2之前，想一次行操作位图的多个位，需要使用管道来进行。但是3.2之后新增了一个强大的指令bitfield， 但一次最多只能操作64位。 使用biefield进行批量操作 bitfield key set u4/i4 value 上面的u代表无符号数，i代表有符号数。 bitfield key get u4 0 代表从第一位取4位，结果时无符号数。 bitfield同时支持incrby自增操作。 birfield中的incrby指令bitfield key incrby u4 2 1代表从第三位开始，对接下来的4位无符号数+1。 如果加法发生上溢出或者下溢出，redis默认的策略时折返，比如1111 加一后溢出则这4位变成0000。 Redis贴心的提供了子指令供我们设置溢出后的策略，默认即是wrap–折返，还提供了fail–失败不执行（返回nil）， sat–饱和截断（保持在最大值）","categories":[],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"}]},{"title":"Redis应用实践——延时队列","slug":"Redis应用实践——延时队列","date":"2019-10-03T04:42:54.000Z","updated":"2019-10-03T04:48:11.825Z","comments":true,"path":"posts/10625/","link":"","permalink":"http://yoursite.com/posts/10625/","excerpt":"","text":"Redis应用实践消息队列 使用list实现 rpush, lpop 实现 客户端判断如果pop出的结果为空 可以sleep 1秒，防止连续不间断的查询浪费性能 使用brpop(blocking)更好的解决上面的问题 如果阻塞太久客户端会自动断开链接，所以代码要编写重试机制 延时队列 使用zset实现，score存放任务的到期处理时间 代码中使用zrangebyscore筛选出已到期的任务 注意多线程时争抢任务的处理，拿到消息后，尝试调用rem执行删除这条消息，返回结果删除成功后才开始执行任务。 优化：可以使用lua脚本将zrangebyscore和zrem变成一个原子操作，避免多次争抢的发生。","categories":[],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"}]},{"title":"Redis应用实践——分布式锁","slug":"Redis应用实践——分布式锁","date":"2019-10-03T04:42:48.000Z","updated":"2019-10-03T04:48:11.825Z","comments":true,"path":"posts/16255/","link":"","permalink":"http://yoursite.com/posts/16255/","excerpt":"","text":"Redis应用实践——分布式锁应用实现分布式锁的开始需要“占坑”， 使用setnx(set if not exists)命令，如果key存在了，其他客户端尝试set时就会报错；等到当前客户端完成任务执行del后，其他客户端方可进入。 为了避免拿到锁之后客户端挂了导致永远不能释放锁，加一个过期时间使用expire指令。 但是setnx 和 expire是两条指令不能原子执行，还是有发生死锁的风险，那怎么解决呢？在Redis 2.8中作者加入了set指令的拓展参数，set lockname true ex 5 nx , 原生支持了原子操作。 超时问题上面的方案如果有些任务执行时间过长，那么释放了锁之后，临界区的代码还未执行完，第二个现成又拿到了锁，那么原来的的代码就不能严格的按照串行执行。所以redis分布式锁不推荐用于耗时较长的任务。 还有要避免其他线程del掉自己的锁，可以使用随机数或者uuid进行匹配，删除时先判断随机数是否一致，不一致不允许删除，但是匹配和删除也不能原子执行，所以这里可以借助lua脚本来执行。 可重入锁java中也有可重入锁的类ReentrantLock，redis实现需要对set方法进行包装，使用ThreadLocal存放key的加锁次数。但是实际项目中最好不要使用可重入锁，可以通过业务的逻辑上调整完全避免使用可重入锁。 补充：可重入锁和不可重入锁参考https://blog.csdn.net/rickiyeat/article/details/78314451","categories":[],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"}]},{"title":"Redis基础数据结构","slug":"Redis基础数据结构","date":"2019-10-03T04:42:43.000Z","updated":"2019-10-03T04:48:11.822Z","comments":true,"path":"posts/53971/","link":"","permalink":"http://yoursite.com/posts/53971/","excerpt":"","text":"Redis基础数据结构基础数据结构string key -value键值对(set, get) 计数 自增自减(incrby) list 链表 可以用左进右出或者右进左出实现队列（rpush, lpop) 右进右出实现栈（rpush, rpop) 慢操作(lindex, ltrim, lrange) 快速链表 hash 相当于java中的HashMap key对应一个hash结构 (hset, hget, hlen, hmset) 扩容时使用渐进rehash避免阻塞 对hash中的单个字段进行计数(hincrby) set 相当于java中的HashSet，即一个所有value为NULL的HashMap 自动去重 sadd, smembers, sismember, spop scard获取长度 zset 可以理解为在set基础上为每个value加一个score字段，自动按有序 zadd, zrange, zrevrange, zscore, zrangebyscore, zrem zcard获取长度 底层使用跳表数据结构，兼顾了查询和插入删除的操作性能。 容器型数据结构的通用规则list, set, hash, zset 都是容器型数据结构，他们同享这两条通用规则。 create if not exists drop if no elements","categories":[],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"}]},{"title":"单链表之查找单链表的中间节点","slug":"单链表之查找单链表的中间节点","date":"2019-07-28T04:14:59.000Z","updated":"2019-07-27T15:13:50.959Z","comments":true,"path":"posts/29610/","link":"","permalink":"http://yoursite.com/posts/29610/","excerpt":"","text":"单链表之查找单链表的中间节点这个实际上之前在反转算法中已经实现过了，这里给大家复习以下思路。 思路使用快指针和慢指针的方法，快指针每次走两步，慢指针每次走一步，当快指针到达尾部的时候，慢指针刚好走过了链表长度的1/2 即指向了中间节点。 代码1234567891011121314151617181920/** * 链表节点个数为奇数时返回中间节点 * 为偶数时返回靠右的中间节点 * @param head * @return */public static ListNode findMiddleNode(ListNode head)&#123; if (head == null || head.next == null) &#123; return head; &#125; ListNode pSlow = head, pFast = head.next; while (pFast != null &amp;&amp; pFast.next != null) &#123; pFast = pFast.next.next; pSlow = pSlow.next; &#125; // pFast == null时为链表个数奇数个，否则为偶数个（取决于快指针最开始从哪里起始） return pFast==null? pSlow:pSlow.next;&#125; 注意：这里会涉及到链表长度是偶数还是奇数的问题，可以看return语句中的三目运算，理解了之后就可以根据自己的需求灵活调整。","categories":[],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"}]},{"title":"单链表之合并两个有序单链表","slug":"单链表之合并两个有序单链表","date":"2019-07-27T14:14:42.000Z","updated":"2019-07-27T15:13:50.958Z","comments":true,"path":"posts/30700/","link":"","permalink":"http://yoursite.com/posts/30700/","excerpt":"","text":"单链表之合并两个有序单链表题目说的是：合并两个有序的单链表，两个链表既然都有序，那其实我们要做的工作就不是很多了，如果两个链表是无序的，那我们还需要各自对两个链表进行排序再进行合并的操作。 思路一：遍历实现这种实现方式思路比较清晰，代码也比较容易理解，但是较为冗长，我自己没有进行实现，大家可以点击链接去作者的博客了解一下。 思路二：递归实现不得不说递归的实现方法真的太巧妙了， 几行代码解决问题。递归比较考验我们的抽象问题的能力。 关于递归，有以下几个要点：1. 问题可以分解为多个相同的小问题。2. 每个小问题的处理方式一样的。 比如我们在电影院，想自己在第几排，那我们可以问前一排的人他在第几排，我们+1就可以了，但是他也不知道在第几排，所以他又去问他前面的人，这样一个接一个问到第一排的人，再一个个返回来。就知道自己第几排了。这就是递归的思路。 合并两个单链表，无非就是不断比较他们当前的节点，将大的（或小的）一个，链接到新的链表上。这里很好的运用了递归把大问题化解成N个小的问题。 12345678910111213141516171819202122232425/** * 使用递归从开始比较两个链表，放入新的链表 * @param h1 * @param h2 * @return */public static ListNode mergeTwoLinkedList(ListNode h1, ListNode h2) &#123; // 这两个属于递归终止条件 if (h1 == null) &#123; return h2; &#125; if (h2 == null) &#123; return h1; &#125; ListNode head = null; if (h1.val &lt;= h2.val) &#123; head = h1; head.next(mergeTwoLinkedList(h1.next, h2)); &#125;else &#123; head = h2; head.next(mergeTwoLinkedList(h1, h2.next)); &#125; return head;&#125; 算法专栏的王争老师在递归这节课中有一句话说的很好：不要试图去详细分析递归的运行过程。 研究过递归代码的同学应该都深有体会，稍稍复杂一点，就很容易看晕，放心，这不是因为我们脑容量不够或者比较笨！","categories":[],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"}]},{"title":"单链表之删除链表中倒数第n个节点","slug":"单链表之删除链表中倒数第n个节点","date":"2019-07-27T14:14:26.000Z","updated":"2019-07-27T15:13:50.957Z","comments":true,"path":"posts/53885/","link":"","permalink":"http://yoursite.com/posts/53885/","excerpt":"","text":"单链表之删除链表中倒数第n个节点这篇依然是学习专栏《数据结构与算法之美》链表相关内容的拓展。 删除链表中倒数第n个节点其实有几种不同的思路， 可以先遍历一遍记下每个节点的位置，最后再遍历找到倒数第n个节点进行删除 学习过时间复杂度分析的同学应该就会发现，这里要遍历两次，在最坏的情况下（要删除的是倒数第一个节点），那么消耗的时间就可以达到2T(n). 使用快慢指针，快指针指向慢指针后的n个节点，然后快慢指针同时前进，直到快指针达到尾部，此时慢指针指向的next就是我们要删除的节点了。 这种方法我刚看到的时候就觉得挺巧妙的，实现也比较简单，下面附上这种解法的代码。 123456789101112131415161718192021222324252627282930 /** * 使用快慢指针，快指针比慢指针多n * 所以当快指针抵达尾部，满指针的next就是准备删除的节点 * @param head * @param n * @return */ public static ListNode deleteNodeFromBack(ListNode head, int n) &#123; // 空链表 if (head == null) &#123; return null; &#125; ListNode pSlow = head; ListNode pFast = head; for (int i = 0; i &lt; n; i++) &#123; pFast = pFast.next; &#125; while (pFast.next != null) &#123; pFast = pFast.next; pSlow = pSlow.next; &#125; // 删除节点 pSlow.next = pSlow.next.next;// System.out.println(pSlow); return head; &#125; 需要注意的是：这里我有些极限情况是没有处理到的，比如n大于了整个链表的长度，为了简洁明了向大家展示这个算法的思路。","categories":[],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"}]},{"title":"单链表之判断表中是否存在环","slug":"单链表之判断表中是否存在环","date":"2019-07-27T14:13:58.000Z","updated":"2019-07-27T15:13:50.959Z","comments":true,"path":"posts/11578/","link":"","permalink":"http://yoursite.com/posts/11578/","excerpt":"","text":"单链表之判断表中是否存在环这篇依然是学习专栏《数据结构与算法之美》的拓展练习。链表中的环其实是一个挺有趣的问题，如果链表中存在环，对其进行遍历的时候就会发现next永远存在节点，循环往复永远不会终止。所以我们在遍历链表之前还是判断链表中是否存在环安全一点。 这篇文章不止讲解了如何判断链表中是否存在环，还贴出了链表中非环部分的长度、环的长度、环的起始点这些算法。 判断链表中是否存在环方法一：使用快慢节点思路学习过其他链表算法的同学应该对快慢指针方法很熟悉了，这里也是使用快慢指针的方法。 原理就是，快指针既然走的比慢指针快，那么当链表中存在环的时候，快慢指针必定在一个位置会相遇，也就是pFast=pSlow。当这个条件一成立我们即可以断定这个链表存在环。 代码1234567891011121314151617181920212223/** * 使用快慢指针， * 快指针每次走两步，慢指针每次走一步 * 如果链表中有环，那两个指针必定有相遇的时候 * @param head * @return */ public static boolean doesLinkedListHasCycle(ListNode head) &#123; if (head == null || head.next == null) &#123; return false; &#125; ListNode pFast = head.next.next,pSlow = head.next; while (pFast.next != null &amp;&amp; pSlow.next != null) &#123; if (pFast == pSlow) &#123; return true; &#125; pFast = pFast.next.next; pSlow = pSlow.next; &#125; return false; &#125; 方法二：使用Map将节点作为key思路这里就是利用的Map的key唯一的特性，遍历链表，当发现map中已经存在当前遍历到的节点时，即我们是再一次来到了这个节点，证明链表存在环。 代码1234567891011121314151617181920212223/** * 使用Map将每一个走过的节点存进去 * 在存之前get一下，如果get有值，说明之前存过，即证明有环 * @param head * @return */public static boolean doesLinkedListHasCycle1(ListNode head) &#123; if (head == null || head.next == null) &#123; return false; &#125; final HashMap&lt;ListNode, Object&gt; map = new HashMap&lt;&gt;(); ListNode p = head; while (p.next != null) &#123; if (map.containsKey(p)) &#123; return true; &#125; map.put(p, null); p = p.next; &#125; return false;&#125; 查找环的起点思路查找环的起点的前提就是链表要存在环，所以此算法的前半部分就是利用了前面的快慢指针判断法，不同的是，当确定存在环的时候我们即跳出循环。 这里有一个推导过程，我参考别人的博客。 推导过程 参考：https://blog.csdn.net/jiaobuchong/article/details/84727007 代码看懂了上面的推导过程，我们直接按照其实现对应的代码就可以了。 123456789101112131415161718192021222324252627282930313233343536373839/** * 查找链表环的起点 * @param head * @return null代表没有环 */public static ListNode findCycleHead(ListNode head) &#123; if (head == null || head.next == null) &#123; return null; &#125; ListNode pFast = head, pSlow = head; // 循环直到快慢指针相遇 while (true) &#123; pFast = pFast.next.next; pSlow = pSlow.next; if (pFast == null) &#123; return null; &#125; if(pFast == pSlow) &#123; break; &#125; &#125; // 让快指针等于头节点，然后各自以1 的步长继续前进，直到再次相遇 // 相遇点即为环的起始点 pFast = head; // 可以顺便计算出非环部分的长度 int i = 0; while (true) &#123; pFast = pFast.next; pSlow = pSlow.next; i++; if (pFast == pSlow) &#123; System.out.println(\"length of not cycle in this LinkedList: \" + i); return pFast; &#125; &#125;&#125; 计算环的长度思路这个算法依然用到了快慢指针，类比到生活中就像是两个人在操场跑步，一个人跑的快一个人跑的慢一点。 我们现在假设快指针叫快男，慢指针叫慢哥 在计算环长度这个算法中，快男和慢男同时在起点起跑，当快男第一次追上慢哥时（即快男已经领先一圈，假设这里时位置a），这是慢哥停下站在原地不动，快男继续跑，当快男再一次追上慢哥的时候（快男转了一圈回到位置a），所以刚刚快男走过的长度就是环的长度。类比到代码里应该就很好理解了。 要注意的是极端条件的处理，防止异常情况的发生，让代码尽可能的健壮 代码12345678910111213141516171819202122232425262728293031323334/** * 计算环的长度，没有环就返回0 * @param head * @return */public static int calculateCycleLength(ListNode head) &#123; if (head == null || head.next == null) &#123; return 0; &#125; ListNode pFast = head, pSlow = head; // 直到相遇 while (true) &#123; pFast = pFast.next.next; pSlow = pSlow.next; if (pFast == null) &#123; return 0; &#125; if (pFast == pSlow) &#123; break; &#125; &#125; // pFast停下，pSlow继续走，直到再次相遇 pSlow走过的长度即为环的长度 int cycleLength = 0; while (true) &#123; pSlow = pSlow.next; cycleLength++; if (pSlow == pFast) &#123; break; &#125; &#125; return cycleLength;&#125;","categories":[],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"}]},{"title":"单链表之：反转算法","slug":"单链表之：反转算法","date":"2019-07-20T07:13:12.000Z","updated":"2019-07-20T07:51:45.774Z","comments":true,"path":"posts/55684/","link":"","permalink":"http://yoursite.com/posts/55684/","excerpt":"","text":"单链表之：链表反转算法上一篇博客讲述了一些链表的知识以及判断是否为回文串的算法，详见如何判断一个单链表是否为回文串？ 一、递归方法思路从头节点开始，递归调用头节点的next，实际上就是从尾节点一次反转各个节点的指针指向。 来源：见参考 实现代码12345678910111213141516/** * 第一种，递归 在反转当前节点之前先反转后续节点 * @param head * @return */ static ListNode reverse0(ListNode head) &#123; if (head ==null || head.next ==null) &#123; return head; &#125; // 递归 从最后一个节点开始反转 ListNode reHead = reverse0(head.next); head.next.next = head;// 自己的下一个的下一个指向自己就是反转过来了 head.next = null;// 清空自己的next指针 相当于断开链表 return reHead; &#125; 二、非递归方法思路从前到后，每两个节点依次进行反转。 来源见参考 实现代码1234567891011121314151617181920212223242526/** * 不使用递归的方法，从前向后进行两两进行交换 * @param head * @return */static ListNode reverse1(ListNode head) &#123; if (head == null || head.next == null) &#123; return head; &#125; ListNode prev = head;// 代表前一个节点 ListNode cur = head.next;// 代表当前节点 ListNode temp = null;// 代表下一个节点 while (cur != null) &#123; temp = cur.next;// 暂存下一个节点，后面赋值回来 cur.next = prev;// 最关键的一句，当前节点next指向前一个节点，即反转了方向 // 移动指针 整体向后移，当前节点变成前节点，下一节点变成当前节点 prev = cur; cur = temp; &#125; head.next = null; return prev;&#125; ListNode12345678910111213141516171819202122public class ListNode &#123; public char val; public ListNode next; public ListNode(char val) &#123; this.val = val; &#125; // builder mode public ListNode next(ListNode next) &#123; this.next = next; return next; &#125; @Override public String toString() &#123; return \"&#123;\" + \"val=\" + val + \", next=\" + next + '&#125;'; &#125;&#125; 测试方法123456 public static void main(String[] args) &#123; ListNode head = new ListNode('a'); head.next(new ListNode('b')).next(new ListNode('d'));// System.out.println(reverse0(head)); System.out.println(reverse1(head)); &#125; 参考：https://blog.csdn.net/guyuealian/article/details/51119499#commentsedit","categories":[],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"}]},{"title":"专栏学习笔记：为什么很多编程语言中数组都从0开始编号？","slug":"专栏学习笔记：为什么很多编程语言中数组都从0开始编号？","date":"2019-07-20T03:14:32.000Z","updated":"2019-07-20T03:44:49.882Z","comments":true,"path":"posts/7261/","link":"","permalink":"http://yoursite.com/posts/7261/","excerpt":"","text":"专栏学习笔记：为什么很多编程语言中数组都从0开始编号？两个概念线性表除了数组，链表、队列、栈等也是线性表结构。 非线性表二叉树、堆、图等都是非线性表，之所以叫非先行是因为 数据在非线性表中并不是简单的前后关系。 数组越界问题在java和c中的差别由于在c语言中，只要不是受限的内存，所有的内存空间都可以是自由访问的。所以当C语言中数组的下标超过了数组的长度，并不会直接出错，而是会出现其他奇怪的问题。 123456789int main(int argc, char* argv[])&#123; int i = 0; int arr[3] = &#123;0&#125;; for(; i&lt;=3; i++)&#123; arr[i] = 0; printf(\"hello world\\n\"); &#125; return 0;&#125; 比如这段C代码，由于循环条件写成了&lt;=3所以 在执行3次以后，数组下标访问越界，这个会造成死循环一致输出hello world问题，这里由于涉及到其他方面的原理，暂不深究，这里贴出作者的讲解和其他读者的评论给大家参考。 而在java中的话，虚拟机就会抛出数组越界异常。 高级语言中的容器以java为例，作为java程序员可能每天都在使用的ArrayList，就是对数组进行一定程度的封装，比如自动扩容，迭代器等 方便程序员的使用。 容器与数组的选择既然容器这么好用，那数组是不是就全无用武之地了呢？ 当然不是，有些时候，使用数组会更加合适。下面列出的几种常见的情况： 由于java的ArrayList只能存储Integer、Float等包装类型，不能存储基本类型， 在自动拆箱装箱会有一定的性能消耗。 如果事先已知数据的大小，那么使用数组会更加方便，在声明数组的时候直接指定好它的大小。研究过ArrayList源代码的同学应该知道，ArraylList底层也是数组，如果不指定初始长度的话，每一次快满了的时候都会自动扩容，而扩容就是使用了数组复制，想象一下，如果你的List有1G的数据，容器扩容到1.5G，就要把这1G的数据复制过去，是不是想想都很费时间。当然，使用ArrayList也可以事先指定好一个大小。关于这一块的源码分析可以看别人的另外一篇文章：https://blog.csdn.net/fighterandknight/article/details/61240861 为什么数组从0开始编号从存储模型的角度来看，数组的下标说是偏移量更为准确，它的英文offset也正是此意。偏移量就表示了相对于首地址偏移了多少，学过C语言的同学应该记得，如果把数组赋给一个变量， 那它的指针就是指向数组的头节点，而头节点的偏移量就是0。是不是很好理解了？","categories":[],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"}]},{"title":"如何判断一个单链表是否为回文串？","slug":"如何判断一个单链表是否为回文串？","date":"2019-07-20T02:58:10.000Z","updated":"2019-07-20T02:59:10.264Z","comments":true,"path":"posts/64872/","link":"","permalink":"http://yoursite.com/posts/64872/","excerpt":"","text":"如何判断一个单链表是否为回文串？前情这道题是极客时间专栏数据结构与算法之美《06 | 链表（上）：如何实现LRU缓存淘汰算法?》里面的一道思考题，同时也是leetcode上的一道题目，原题为：https://leetcode.com/problems/palindrome-linked-list/ 由于我在算法方面的基础比较薄弱，所以直接去看了别人的解决方法，也花了好一会才看懂，加上照着敲一遍代码，跑debug 最终才完全理解了这种解法。 思路使用快慢两个指针找到链表中点，慢指针每次前进一步，快指针每次前进两步。在慢指针前进的过程中，同时修改其 next 指针，使得链表前半部分反序。最后比较中点两侧的链表是否相等。 代码里面的注释都是我在思考过后补充上去的，相信大家理解了之后再删掉注释也会感觉豁然开朗。 我自己给ListNode使用了builder模式，方便测试的时候输入数据（其实不算是严格的builder模式，真正的builder是返回对象this，这里是返回了next的引用） 我的注释可能影响你的思考，可以尝试忽略我的注释或者删掉，自己思考 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475/** * Given a singly linked list, determine if it is a palindrome. */public class PalindromeLinkedList &#123; // solution static boolean isPalindrome(ListNode head) &#123; if (head == null || head.next == null) &#123; return true; &#125; ListNode prev = null; ListNode fast = head; ListNode slow = head; while (fast != null &amp;&amp; fast.next != null) &#123; // 每次跳两个 变相控制了循环的次数，相当于除2 fast = fast.next.next; // 临时变量 暂存 ListNode next = slow.next; // 一开始prev为空时：slow变成了头节点；到后面就是把本来在prev的元素变成它的next slow.next = prev; prev = slow; slow = next; &#125; // 循环结束 prev就是链表前半部分反序后的链表，slow就是原链表的后半部分 // 链表节点为偶数时刚好除尽，为奇数时这里的fast就是最后一个节点 // 节点个数为奇数时则不用比较中间节点 所以这里.next跳过了中间节点 if (fast != null) &#123; slow = slow.next; &#125; // 依次比较，如果有不同直接返回false while (slow != null) &#123; if (slow.val != prev.val) &#123; return false; &#125; slow = slow.next; prev = prev.next; &#125; return true; &#125; public static void main(String[] args) &#123; ListNode head = new ListNode('a'); head.next(new ListNode('b')).next(new ListNode('a')); System.out.println(head); System.out.println(isPalindrome(head)); &#125;&#125;class ListNode &#123; char val; ListNode next; public ListNode(char val) &#123; this.val = val; &#125; // builder mode ListNode next(ListNode next) &#123; this.next = next; return next; &#125; @Override public String toString() &#123; return \"&#123;\" + \"val=\" + val + \", next=\" + next + '&#125;'; &#125;&#125; 运行结果","categories":[],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://yoursite.com/tags/数据结构与算法/"}]},{"title":"深入理解Tomcat之一：Tomcat基础架构","slug":"深入理解Tomcat之一：Tomcat基础架构","date":"2019-07-06T09:26:18.000Z","updated":"2019-07-06T09:00:30.177Z","comments":true,"path":"posts/24480/","link":"","permalink":"http://yoursite.com/posts/24480/","excerpt":"","text":"深入理解Tomcat之一：Tomcat基础架构Tomcat的定义（来自wiki百科）Tomcat是由Apache软件基金会下属的Jakarta项目开发的一个Servlet容器，按照Sun Microsystems提供的技术规范，实现了对Servlet和JavaServer Page（JSP）的支持，并提供了作为Web服务器的一些特有功能，如**Tomcat管理和控制平台、安全域管理和Tomcat阀等。 由于Tomcat本身也内含了一个HTTP服务器，它也可以被视作一个单独的Web服务器。 Http服务器与应用服务器之前在技术讨论群里看到一个群友提问：nginx和tomcat有什么区别呀？其实提出这种问题就可以得知他对两个服务器的基础概念都还不是很清晰。 严格来讲，Nginx、Apache这些叫做Http Server； 而Tomcat是Application Server，更准确的说，是一个Servlet、Jsp容器。 一个 HTTP Server 关心的是 HTTP 协议层面的传输和访问控制，所以在 Apache/Nginx 上你可以看到代理、负载均衡等功能。客户端通过 HTTP Server 访问服务器上存储的资源（HTML 文件、图片文件等等）。一个 HTTP Server 始终只是把服务器上的文件如实的通过 HTTP 协议传输给客户端。 对于 Tomcat 来说，就是需要提供 JSP/Sevlet 运行需要的标准类库、Interface 等。为了方便，应用服务器往往也会集成 HTTP Server 的功能，但是不如专业的 HTTP Server 那么强大，所以应用服务器往往是运行在 HTTP Server 的背后，执行应用，将动态的内容转化为静态的内容之后，通过 HTTP Server 分发到客户端。 回到上面的问题，在我的理解看来，Nginx更像一个协调管理者的角色，而Tomcat用“容器”这个词语形容它会更加形象贴切一点。 Tomcat的整体架构 Tomcat最顶层的容器是Server， 代表整个服务器，从上图可以看出一个 Server至少包含 一个Service，用于具体提供服务。 两个主要组件Connecter用于处理连接相关的事，并提供Socket与Request和Response相关的转化; 基本功能一个Connecter将在某个指定的端口上侦听客户请求，接收浏览器的发过来的 tcp 连接请求，创建一个 Request 和 Response 对象分别用于和请求端交换数据，然后会产生一个线程来处理这个请求并把产生的 Request 和 Response 对象传给处理Engine(Container中的一部分)，从Engine出获得响应并返回客户。 Tomcat中有两个经典的Connector， 一个直接侦听来自Browser的HTTP请求， 另外一个来自其他的WebServer请求。 HTTP/1.1 Connector在端口8080处侦听来自客户Browser的HTTP请求，AJP/1.3 Connector在端口8009处侦听其他Web Server（其他的HTTP服务器）的Servlet/JSP请求。 Connector 最重要的功能就是接收连接请求然后分配线程让 Container 来处理这个请求，所以这必然是多线程的，多线程的处理是 Connector 设计的核心。 Connector 中具体是用ProtocolHandler 来处理请求的，不同的ProtocolHandler 代表不同的连接类型，比如， Http11Protocol 使用的是普通Socket 来连接的， Http 11 NioProtocol 使用的是NioSocket 来连接的。 ProtocolHandler 里面有3 个非常重要的组件： Endpoint 、Processor 和Adapter。 Endpoint用于处理底层Socket 的网络连接， Acceptor 用于监昕请求 AsyncTimeout 用于检查异步request 的超时 Handler 用于处理接收到的Socket，在内部调用了Processor 进行处理。 Processor 用于将Endpoint 接收到的Socket 封装成Request, Adapter 用于将封装好的Request 交给Container 进行具体处理。 也就是说Endpoint用来实现TCP/IP 协议， Processor 用来实现HTTP 协议， Adapter 将请求适配到Servlet 容器进行具体处理。 Container用于封装和管理Servlet，以及具体处理Request请求； Container用于封装和管理Servlet，以及具体处理Request请求，在Connector内部包含了4个子容器。 4个子容器的作用分别是： （1）Engine：引擎，用来管理多个站点，一个Service最多只能有一个Engine； （2）Host：代表一个站点，也可以叫虚拟主机，通过配置Host就可以添加站点； （3）Context：代表一个应用程序，对应着平时开发的一套程序，或者一个WEB-INF目录以及下面的web.xml文件； （4）Wrapper：每一Wrapper封装着一个Servlet； 其他组件 Jasper：负责jsp页面的解析，jsp属性的验证,同时负责将jsp动态转换为java代码并编译成class。 Naming：资源管理，负责数据库连接池、EJB、mail等通过JDNI获取的内容。 Session：会话管理的组件 Logging：日志相关 JMX：性能监控等","categories":[],"tags":[]},{"title":"深入理解Tomcat之二：自己动手实现一个简单的Tomcat","slug":"深入理解Tomcat之二：自己动手实现一个简单的tomcat","date":"2019-07-06T09:26:18.000Z","updated":"2019-07-06T09:00:30.178Z","comments":true,"path":"posts/6083/","link":"","permalink":"http://yoursite.com/posts/6083/","excerpt":"","text":"深入理解Tomcat之二：自己动手实现一个简单的Tomcat学习一个新的知识的过程就是 看别人的文章、听别人讲、自己查资料、自己给别人讲。我们对于新知识的认识成都以及理解深度都是在整个过程中不断的加深的。所以我一直提倡大家要乐于分享，当你给别人用组织系统化的语言或者文章将你头脑中的知识输出来，你就会发现，你对之前知识的理解又加深了一个程度。 而这一篇手写tomcat，其实也是我在学习tomcat架构的过程中，模仿别人的代码自己再手敲一遍，最后将思路和实现过程整理成文，输出给大家。 主要需求 监听请求端口 封装请求和返回 对请求进行处理 上面就是mini tomcat的类图 各个类MyRequest自己封装的请求类，相当于servlet中的HttpRequest。 inputStream来自于socket的输入流，用浏览器访问的时候就会包含了整个请求的报文 解析http请求头的第一行 拿出协议中的 GET 或者 POST 还有请求url 1234567891011121314151617181920212223242526272829303132333435363738394041/** * 自己封装的请求 */public class MyRequest &#123; private String url; private String method; public MyRequest(InputStream inputStream) throws IOException &#123; String httpRequest = \"\"; byte[] requestBytes = new byte[1024]; int length = inputStream.read(requestBytes); if(length&gt;0)&#123; httpRequest = new String(requestBytes, 0, length); &#125; // 解析内容 // 第一行是http头部内容 String httpHead = httpRequest.split(\"\\n\")[0]; url = httpHead.split(\"\\\\s\")[1]; method = httpHead.split(\"\\\\s\")[0]; System.out.println(\"接收到请求-------》\"); System.out.println(\"请求信息 ：\" + toString()); &#125; @Override public String toString() &#123; return \"url: \" + url + \", method: \" + method; &#125; public String getUrl() &#123; return url; &#125; public String getMethod() &#123; return method; &#125;&#125; MyResponse 要点在于手动按照http协议的格式进行响应，这样浏览器才可以识别 1234567891011121314151617181920212223242526272829/** * 自己封装的响应 */public class MyResponse &#123; private OutputStream outputStream; public MyResponse(OutputStream outputStream) &#123; this.outputStream = outputStream; &#125; public void write(String content) throws IOException &#123; if(outputStream!=null) &#123; StringBuffer httpResponse = new StringBuffer(); // 按照http响应格式输出 httpResponse.append(\"HTTP/1.1 200 OK\\n\") .append(\"Content-Type: text/html\\n\") .append(\"\\r\\n\") .append(\"&lt;html&gt;&lt;body&gt;\") .append(content) .append(\"&lt;/body&gt;&lt;/html&gt;\"); System.out.println(\"返回信息：\"+httpResponse.toString()); outputStream.write(httpResponse.toString().getBytes()); outputStream.close(); &#125; &#125;&#125; MyServlet抽象的Servlet，可以继承它来有很多不同的实现。 service 方法根据请求的方法分发到get或者post进行处理，这里与 12345678910111213141516171819/** * 自己封装的Servlet * 继承后做不同的实现 */public abstract class MyServlet &#123; public abstract void doGet(MyRequest httpRequest, MyResponse httpResponse); public abstract void doPost(MyRequest httpRequest, MyResponse httpResponse); public void service(MyRequest request, MyResponse response) &#123; if(request!=null) &#123; if (\"POST\".equalsIgnoreCase(request.getMethod())) &#123; doPost(request, response); &#125; else if (\"GET\".equalsIgnoreCase(request.getMethod())) &#123; doGet(request, response); &#125; &#125; &#125;&#125; HelloServlet1234567891011121314151617181920212223/** * Servlet的实现类，在post get中添加具体的业务逻辑 * 以前使用servlet编程的时候也是这样的实现方法 */public class HelloServlet extends MyServlet &#123; @Override public void doGet(MyRequest httpRequest, MyResponse httpResponse) &#123; try &#123; httpResponse.write(\"get method in hello servlet !!\"); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; @Override public void doPost(MyRequest httpRequest, MyResponse httpResponse) &#123; try &#123; httpResponse.write(\"post method in hello servlet\"); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; ServletMapping其实是一个Bean，简单封装了配置信息，方便我们读取。 123456789101112131415161718192021222324252627282930313233343536public class ServletMapping &#123; private String servletName; private String url; private String clazz; public ServletMapping(String servletName, String url, String clazz) &#123; this.servletName = servletName; this.url = url; this.clazz = clazz; &#125; public String getServletName() &#123; return servletName; &#125; public void setServletName(String servletName) &#123; this.servletName = servletName; &#125; public String getUrl() &#123; return url; &#125; public void setUrl(String url) &#123; this.url = url; &#125; public String getClazz() &#123; return clazz; &#125; public void setClazz(String clazz) &#123; this.clazz = clazz; &#125;&#125; ServletMappingConfig存着一个列表来保存配置，真正tomcat也不是这样实现的。我们只是为了效果方便实现。 1234567891011/** * 这里由于是demo 采用简洁的方式先进行配置 */public class ServletMappingConfig &#123; public static List&lt;ServletMapping&gt; config = new ArrayList&lt;ServletMapping&gt;(); // 在真正的tomcat中是扫描web.xml的配置来初始化ServletMapping static &#123; config.add(new ServletMapping(\"hello\", \"/hello\", \"com.practice.HelloServlet\")); &#125;&#125; MyTomcatminitomcat的核心类。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293public class MyTomcat &#123; private int port = 8080; private HashMap&lt;String, String&gt; mapping = new HashMap&lt;String, String&gt;(); public MyTomcat(int port) &#123; this.port = port; &#125; /** * 使用socket不断等候接收新的请求 */ void start() &#123; initservletMappings(); ServerSocket socket = null; try &#123; socket = new ServerSocket(port); System.out.println(\"Tomcat 启动成功～～\"); while (true) &#123; Socket accept = socket.accept(); InputStream inputStream = accept.getInputStream(); OutputStream outputStream = accept.getOutputStream(); MyRequest myRequest = new MyRequest(inputStream); MyResponse myResponse = new MyResponse(outputStream); dispatch(myRequest, myResponse);// socket.close(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; if (socket != null) &#123; try &#123; socket.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; /** * 读取配置文件初始化servletMapping */ private void initservletMappings() &#123; for (ServletMapping servletMapping : ServletMappingConfig.config) &#123; mapping.put(servletMapping.getUrl(), servletMapping.getClazz()); &#125; &#125; /** * 读取配置找到对应的Servlet类，使用反射创建实例处理请求 * @param request * @param response */ private void dispatch(MyRequest request, MyResponse response) &#123; String url = request.getUrl(); String clazz = mapping.get(url); if(clazz==null || clazz==\"\")&#123; System.out.println(\"没有找到请求对应的链接:\" + url); System.out.println(); return; &#125; try &#123; Class&lt;MyServlet&gt; aClass = (Class&lt;MyServlet&gt;) Class.forName(clazz); MyServlet myServlet = aClass.newInstance(); myServlet.service(request, response); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); System.out.println(\"不存在该类\"); &#125; catch (IllegalAccessException e) &#123; e.printStackTrace(); &#125; catch (InstantiationException e) &#123; e.printStackTrace(); &#125; &#125; /** * 启动tomcat * @param args */ public static void main(String[] args) &#123; new MyTomcat(8081).start(); &#125;&#125; 最后我们运行main方法，在浏览器中访问localhost:8081/hello 就可以看到效果啦！ 总结这个tomcat很mini ，只简单实现了基本的功能，大家可以在这个基础上不断添加其他的功能，让这个minitomcat越来越接近真正的tomct！！","categories":[],"tags":[]},{"title":"Spring中的事件简述与Guava的EventBus","slug":"Spring中的事件简述与Guava的EventBus","date":"2019-06-11T09:26:18.000Z","updated":"2019-06-11T09:27:22.325Z","comments":true,"path":"posts/38349/","link":"","permalink":"http://yoursite.com/posts/38349/","excerpt":"","text":"Spring中的事件简述与Guava的EventBusSpring的事件关键类 org.springframework.context.ApplicationEvent org.springframework.context.ApplicationListener 使用容器触发事件，applicationContext发布事件。 简单实现逻辑 自定义订阅和通知事件，继承ApplicationEvent 定义事件监听器，实现ApplicationListener 使用容器发布事件（订阅、通知） 拓展@EventListener注解为了加强@EventListener的功能，Spring 4.2开始支持使用SpEL表达式定义事件触发的条件。 下面为使用了该注解的的一个实例： Event： 123456789101112131415161718192021222324public class TestEvent extends ApplicationEvent &#123; public boolean isImport; public TestEvent(Object source, boolean isImport) &#123; super(source); this.isImport = isImport; &#125; public boolean isImport() &#123; return isImport; &#125; public void setImport(boolean anImport) &#123; isImport = anImport; &#125; @Override public String toString() &#123; return \"TestEvent&#123;\" + \"isImport=\" + isImport + '&#125;'; &#125;&#125; Listener: 123456789@Componentpublic class EventHandler &#123; // 当isImport为true的时候才会打印 @EventListener(condition=\"#testEvent.isImport\") public void TestEventTest(TestEvent testEvent) &#123; System.out.println(\"==============TestEvent==============\" + testEvent.toString()); &#125;&#125; 拓展Google Guava中的EventBusGoogleGuava是谷歌在日常开发过程中总结积累出来的一个类库，包含了许多常用的工具等。 Guava的优点： 高效设计良好的API，被Google的开发者设计，实现和使用 遵循高效的java语法实践 使代码更刻度，简洁，简单 节约时间，资源，提高生产力 Guava工程包含了若干被Google的 Java项目广泛依赖 的核心库，例如： 集合 [collections] 缓存 [caching] 原生类型支持 [primitives support] 并发库 [concurrency libraries] 通用注解 [common annotations] 字符串处理 [string processing] I/O 等等。 这里我们主要介绍Guava中的事件总线EventBus。使用Guava的事件总线就不用再像上面Spring中的继承实现接口的方法。只需要在指定的事件处理方法上加@Subscribe注解即可。 消息封装类： 12345678910public class TestEvent &#123; private final int message; public TestEvent(int message) &#123; this.message = message; System.out.println(\"event message:\"+message); &#125; public int getMessage() &#123; return message; &#125;&#125; 消息接收类： 12345678910111213public class EventListener &#123; public int lastMessage = 0; @Subscribe public void listen(TestEvent event) &#123; lastMessage = event.getMessage(); System.out.println(\"Message:\"+lastMessage); &#125; public int getLastMessage() &#123; return lastMessage; &#125;&#125; 测试类及输出结果： 12345678910111213141516171819202122232425public class TestEventBus &#123; @Test public void testReceiveEvent() throws Exception &#123; EventBus eventBus = new EventBus(\"test\"); EventListener listener = new EventListener(); eventBus.register(listener); eventBus.post(new TestEvent(200)); eventBus.post(new TestEvent(300)); eventBus.post(new TestEvent(400)); System.out.println(\"LastMessage:\"+listener.getLastMessage()); &#125;&#125;//输出信息event message:200Message:200event message:300Message:300event message:400Message:400LastMessage:400 以上即是EventBus的简单使用。 参考资料:https://shimo.im/docs/z7ggA56biOAfAdht/read https://www.cnblogs.com/peida/p/EventBus.html","categories":[],"tags":[]},{"title":"关于二分查找（折半查找）的即记录","slug":"关于二分查找（折半查找）的即记录","date":"2019-06-05T07:36:18.000Z","updated":"2019-06-05T07:47:40.675Z","comments":true,"path":"posts/1/","link":"","permalink":"http://yoursite.com/posts/1/","excerpt":"","text":"关于二分查找（折半查找）的即记录之前其实也学习过二分查找的理念，但是一直有一个模糊的地方就是中间元素的选取。 二分查找需要的数组需要是有序的。 二分查找的步骤 确定数组的中间元素 将待查找元素与中间元素比较 如果大于中间元素，则到右边的数组查找，反之同理 如果中间元素等于待查找元素，那么查找成功。 之前一直迷惑的一点就是第一步当中的中间元素的选取，因为数组会有两种情况，一种是元素的个数为偶数，另一种是数组元素个数为奇数。 今天到网上查找相关资料，才补上了这个基础的知识： 中间元素的选取可以使用如下公式：mid = left + (right - left)/2; 这样不论当数组元素个数为偶数或者奇数的时候都可以正确选取到中间元素。 二分查找的思想其实二分查找是利用的算法设计思想中的 分治法，一步一步缩小查找范围，最终得到问题的解。 分治法：将一个复杂的问题分解为多个相同或相似的子问题，再对子问题进行求解，进而得到问题的解。 二分法的求解过程可以用二叉树来描述，对于一个有序的数组，根结点为最开始选取的中间元素，根结点的左右两个孩子分别为左数组的中间节点及右数组的中间节点，孩子的孩子同理； 所以通过查找树（判定树）可以看出查找的元素要经过几次比较以及跟哪些元素进行了比较。 二分查找的时间复杂度O(log2n)","categories":[],"tags":[]},{"title":"关于token机制与JWT标准","slug":"关于token机制与JWT标准","date":"2019-06-05T07:36:18.000Z","updated":"2019-06-05T07:49:13.679Z","comments":true,"path":"posts/33574/","link":"","permalink":"http://yoursite.com/posts/33574/","excerpt":"","text":"关于token机制与JWT标准在传统的web单体项目中，我们的会话通常使用session和cookie两样来实现。 传统的cookie/session实现session是保存在服务器端的会话，使用cookie在客户端保存一个session id，每次发送请求的时候带上这个cookie服务端就可以很容易的了解当前用户是否与服务端建立了会话，以及会话是否过期等等。在单体项目中的这种实现机制相对来说是比较方便的。 使用这种机制存在的问题： 当我们的项目变得越来越大，用户数量越来越多之后，每新建一个会话服务器就要多消耗空间来存储空间，这将使服务器内存的开销不断增加； 跨域问题，很多项目使用前后端分离开发，而且前后端也部署在不同的域，这将导致一系列跨域问题的发生； 可拓展性，只在服务器内存中存储session限制了可拓展性能； 针对以上痛点，token机制的出现很好的解决了问题。 token机制使用token机制时，当用户请求时，会检查用户是否有携带token信息，或者token是否过期；未登录的话叫用户进行登录，然后服务端生成一串token返回给浏览器，可以存储在cookie或者localStorage里面。以后用户每一次发送的请求都会带上这个token。 token的组成一般是一些基础的信息，加上一个签名。这样服务器就无需存储session，每当收到一个新的请求，对token的签名进行验证，验证成功就可以进行下一步的操作。而且通常token可以存储在数据库中。 JWT（Json Web Token）广义上的jwt其实是token机制的一个实现标准，狭义上的jwt即是我们每次请求携带的token串。 jwt串由3部分组成，下面是一个例子： 123eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c 第一部分为header为json格式 1234&#123; \"alg\": \"HS256\", 加密算法类型 \"typ\": \"JWT\" token类型&#125; 第二部分为payload 同样为json： 12345&#123; &quot;sub&quot;: &quot;1234567890&quot;, 保留claim subject代表这个jwt的主体 &quot;name&quot;: &quot;John Doe&quot;, 自定义的claim &quot;iat&quot;: 1516239022 保留claim 时间戳 代表jwt的签发时间&#125; 第三部分就是用header中指定的算法结合payload的内容制造出的签名。 jwt串只是使用base64编码，并没有进行加密，所以任何人获取到jwt串都可以拿到其中传输的内容。 当服务端获取到这一tokne之后，会有如下事件流： 使用base64将jwt串进行还原 使用header中的算法对签名部分进行还原 对比直接从jwt中解析出的payload部分和使用算法还原的payload 如果不一致 验证失败 一致则验证成功 总结token机制是针对无状态的http比较好的一个会话管理机制；对前后端分离项目、用户量大的项目都更加友好；对移动端的支持也更好。 参考资料： https://jwt.io/ https://www.cnblogs.com/moyand/p/9047978.html https://www.cnblogs.com/lyzg/p/6028341.html","categories":[],"tags":[]},{"title":"深入理解MyBatis（一）","slug":"深入理解MyBatis（一）","date":"2019-06-05T07:36:18.000Z","updated":"2019-06-05T07:48:06.690Z","comments":true,"path":"posts/4/","link":"","permalink":"http://yoursite.com/posts/4/","excerpt":"","text":"深入理解MyBatis（一）MyBatis的基本运行过程 输入配置文件的文件流 使用SqlSessionFactoryBuilder根据配置文件创建SqlSessionFactory 通过SqlSessionFactory获取会话SqlSession 从SqlSession获取Executor Executor读取并执行sql语句 StatementHandler处理jdbc的statement交互 TypeHandler负责设置参数 使用jdbc与数据库进行交互 jdbc返回结果集-》TypeHandler-》ResultHandler-》StatementHandler-》Executor-》SqlSession 上述过程使用图片来描述会比较清晰。 MyBatis的缓存机制一级缓存Executor在Mybatis中扮演者非常重要的一个角色，除了用来控制执行sql返回结果之外，它还有一个重要的指责就是缓存的管理。 在一个SqlSession的会话中，如果使用了同样的sql以及同样的参数两次以上，那么在第二次查询的时候就会命中mybatis的一级缓存，不再次查库； 在这个session会话中，如果有进行增删改操作，那么mybatis就会刷新缓存避免脏读现象的发生； 一级缓存在Mybatis中是默认开启的。 二级缓存二级缓存在Mybatis中其实也是默认开启的，二级缓存对应的配置项为&lt;setting name=&quot;cacheEnabled&quot; value=&quot;true&quot;/&gt; ，但是如果想要使用二级缓存需要在对应的mapper文件中加上如下配置 12&lt;!-- 开启mapper的二级缓存， type:指定cache接口的实现类，mybatis默认使用PerpetualCache 要和ehcache整合，需要配置type为ehcache实现cache接口的类型 --&gt;&lt;cache type=\"org.mybatis.caches.ehcache.EhcacheCache\"&gt;&lt;/cache&gt; 如果不需要指定忽略type参数就可以了: &lt;cache/&gt;。 关于二级缓存的划分：既可以每个mapper使用自己的空间，也可以多个mapper共享一个空间（使用&lt;cache-ref namespace=&quot;xx&quot;/&gt;来配置）。 Mybatis对二级缓存的粒度控制很细，所以如果想要使用二级缓存，需要在指定的select标签中开启对该条语句进行缓存cacheEnabled=true。 实现原理Mybatis实现二级缓存使用了装饰器模式，使用CachingExecutor装饰了Executor；从上文我们可以知道Executor控制了一级缓存，所以我们可以发现在一二级缓存同时开启式，mybatis是会优先使用二级缓存的。 最后：关于实际应用中的缓存原则：尽量离客户端近；能用cdn的就cdn，能在nginx缓存的就在nginx缓存，接下来在控制层，业务层进行缓存，基本上在实际应用中不会使用到数据库级别的缓存。","categories":[],"tags":[]},{"title":"如何给自己的网站添加小绿锁","slug":"如何给自己的网站添加小绿锁","date":"2019-06-05T07:36:18.000Z","updated":"2019-06-05T07:47:46.773Z","comments":true,"path":"posts/2/","link":"","permalink":"http://yoursite.com/posts/2/","excerpt":"","text":"如何给自己的网站添加小绿锁首先 要值得庆祝的是我的网站通过备案了 之前我的博客是用的hexo博客，项目托管在github.io上，在本地使用Markdown写好文章 编译发布到git仓库就可以了。因此也无需备案。不过发现，github的访问速度不稳定，时快时慢就比较坑爹 前一阵刚好赶上了阿里云的双11活动，直接298买了个最低配的3年，寻思买来放放博客，自己的小项目什么的都挺好。刚好朋友最近推荐了一个java的开源博客项目 就是现在用的tale，而且hexo看久了 也腻了 而hexo且没有后台，要手写markdown然后手动编译再git push 挺麻烦的。所以我就在阿里云上搭了这个tale。功能比较简洁明了，用的是内置sqlite，总共没有几张表。不依赖容器。如何安装tale大家直接点连接进去看就可以了，很简单 傻瓜式操作。 接下来讲讲如何给你的网站添加小绿锁 以下提到两个证书供应商都提供免费的ssl DV证书，letsencrypt三个月需要续期，可以自动续期，阿里云免费一年。 使用letsencrypt给站点添加ssl比较热门和方便的一个方法就是使用letsencrypt的Certbot自动给你的站点添加证书。进入到certbot网站可以看到支持的服务器以及系统，本人推荐使用nginx，配置比较方便。大概过程就是连接到你的服务器，安装certbot，配置一下nginx就ok了。这里不多讲certbot的使用了，因为我用的是阿里云的证书。 阿里云ssl证书如果你的域名也是在阿里云购买的话，这个过程就会更加方便。 在阿里云购买的域名–》直接进入阿里云控制台 –域名 点击管理，里面有申请免费的ssl证书。 不是在阿里云购买的域名就要在阿里云搜索ssl进入根据提示申请一个免费的ssl证书，这里相较于 1 麻烦的就是要手动验证域名的所有权。可能要添加一条text类型的解析记录。 现在假定你已经申请好了证书，可以看到证书管理界面你的证书那里有个下载，点击他，下载完你会拿到一个压缩包，里面有量个主要的文件，一个是以crt结尾的证书文件，一个是以key结尾的私钥文件。 剩下的步骤 阿里云的官方文档已经讲的很清楚了，详见：Nginx/Tengine服务器安装SSL证书。 Tomcat的也有，详见：Tomcat服务器安装SSL证书 有一点要注意的是https使用的是443端口，http默认是80端口，所以要到阿里云的安全组添加开放你的443端口 后续配置做完上边的工作，你的网站应该就可以使用https://your.domain来访问了~ 但细心的小伙伴会发现一个问题，这里一定要手动加上https的头，为什么人家的网站都可以直接使用域名访问自动跳转到https呢，其实这里也是后台要配置一个转发，将http的请求全部全部转到https。下边是我完整的nginx配置文件 供大家参考。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182# For more information on configuration, see:# * Official English Documentation: http://nginx.org/en/docs/# * Official Russian Documentation: http://nginx.org/ru/docs/user nginx;worker_processes auto;error_log /var/log/nginx/error.log;pid /run/nginx.pid;# Load dynamic modules. See /usr/share/nginx/README.dynamic.include /usr/share/nginx/modules/*.conf;events &#123; worker_connections 1024;&#125;http &#123; log_format main &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos; &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;; access_log /var/log/nginx/access.log main; sendfile on; tcp_nopush on; tcp_nodelay on; keepalive_timeout 65; types_hash_max_size 2048; include /etc/nginx/mime.types; default_type application/octet-stream; # Load modular configuration files from the /etc/nginx/conf.d directory. # See http://nginx.org/en/docs/ngx_core_module.html#include # for more information. include /etc/nginx/conf.d/*.conf; server &#123; listen 80 default_server; listen [::]:80 default_server; server_name wasea.top www.wasea.top; root /usr/share/nginx/html; rewrite ^(.*)$ https://$host$1 permanent; # 重要的一个配置，可以自动转到https # Load configuration files for the default server block. include /etc/nginx/default.d/*.conf; location / &#123; proxy_pass http://localhost:9000; # 我的tale博客默认使用9000端口 &#125; error_page 404 /404.html; location = /40x.html &#123; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; &#125; &#125;# start aliyun ssl configserver &#123;# listen 443; listen 443 ssl http2 default_server; listen [::]:443 ssl http2 default_server; server_name wasea.top www.wasea.top; ssl on; root html; index index.html index.htm; ssl_certificate cert/a.pem; ssl_certificate_key cert/a.key; ssl_session_timeout 5m; ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on; location / &#123; add_header Content-Security-Policy upgrade-insecure-requests; proxy_pass http://localhost:9000; &#125;&#125;# end aliyun ssl config&#125; 还有一点值得说的是，我这里省略了linux下一些细节的操作命令，例如给防火墙开放端口、安装nginx 等等其他，这些每个系统都有细微的区别，大家根据自己的系统来操作就好了。","categories":[],"tags":[]},{"title":"跨域资源共享CORS 详解","slug":"跨域资源共享CORS 详解","date":"2019-06-05T07:36:18.000Z","updated":"2019-06-05T07:48:18.440Z","comments":true,"path":"posts/5/","link":"","permalink":"http://yoursite.com/posts/5/","excerpt":"","text":"跨域资源共享CORS 详解http://www.ruanyifeng.com/blog/2016/04/cors.html","categories":[],"tags":[]},{"title":"小工具：将传入的日期区间按日、周、月、季、年分隔成多个时间区间","slug":"小工具：将传入的日期区间按日、周、月、季、年分隔成多个时间区间","date":"2019-02-03T04:33:18.000Z","updated":"2019-06-05T07:48:44.857Z","comments":true,"path":"posts/3/","link":"","permalink":"http://yoursite.com/posts/3/","excerpt":"","text":"小工具：将传入的日期区间按日、周、月、季、年分隔成多个时间区间最近在做公司项目的时候，有一个需求是需要传入一个开始时间和一个结束时间。但是要按照统计的方式分别以日、周、月、季、年的方式来分组进行统计。这个问题其实有两个解决方案，一个是利用sql处理日期的字段格式化进行分组；另一种就是写一个通用一点的SQL，然后在service里处理这些日期，遍历调用这个接口。由于我们的需求对时间的要求比较复杂，并不可以按照自然月的方式进行统计，而是上个月26到本月25日为一个周期。比如统计2019年1月，实际上的开始和结束时间为：2018-12-26~2019-01-25。也正是因为这点，使用sql分组的方式就更加难以实现。所以我选用了第一个解决方案，就是在业务里处理这些时间。遂萌生了写这个工具类的想法。因为我们的业务需求比较特殊，所以下边贴上的代码还是按照自然月来处理的。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272import java.text.SimpleDateFormat;import java.util.*;/** * 传开始和结束日期 按日、周、月、季、年分割成一些时间区间 * &lt;p&gt; */public class DateSeparateUtil &#123; /** * 按日分隔 * * @param startTime * @param endTime * @param format * @param timeForQuery 分隔好的时间区间 一个startTime和endTime为一组 * @param timeForShow 显示在横轴的时间区间 */ public static void separateByDay(Date startTime, Date endTime, SimpleDateFormat format, List&lt;Map&lt;String, String&gt;&gt; timeForQuery, List&lt;String&gt; timeForShow) &#123; if (timeForQuery == null) &#123; timeForQuery = new ArrayList&lt;&gt;(); &#125; if (timeForShow == null) &#123; timeForShow = new ArrayList&lt;&gt;(); &#125; Calendar start = Calendar.getInstance(); start.setTime(startTime); Calendar end = Calendar.getInstance(); end.setTime(endTime); while (start.getTimeInMillis() &lt; end.getTimeInMillis()) &#123; HashMap&lt;String, String&gt; map = new HashMap&lt;&gt;(); String startStr = format.format(new Date(start.getTimeInMillis())); map.put(\"startTime\", startStr + \" 00:00:00\"); // 按天算的话 开始和结束就是同一天 map.put(\"endTime\", startStr + \" 23:59:59\"); timeForQuery.add(map); timeForShow.add(startStr.substring(5, 10)); start.add(Calendar.DAY_OF_YEAR, 1); &#125; &#125; /** * 按星期分隔 * * @param startTime * @param endTime * @param format * @param timeForQuery * @param timeForShow */ public static void separateByWeek(Date startTime, Date endTime, SimpleDateFormat format, List&lt;Map&lt;String, String&gt;&gt; timeForQuery, List&lt;String&gt; timeForShow) &#123; if (timeForQuery == null) &#123; timeForQuery = new ArrayList&lt;&gt;(); &#125; if (timeForShow == null) &#123; timeForShow = new ArrayList&lt;&gt;(); &#125; Calendar start = Calendar.getInstance(); start.setFirstDayOfWeek(Calendar.MONDAY); start.setTime(startTime); Calendar end = Calendar.getInstance(); end.setTime(endTime); while (start.getTimeInMillis() &lt; end.getTimeInMillis()) &#123; HashMap&lt;String, String&gt; map = new HashMap&lt;&gt;(); String startStr = format.format(new Date(start.getTimeInMillis())); map.put(\"startTime\", startStr + \" 00:00:00\"); start.add(Calendar.DAY_OF_WEEK, /*start.getActualMaximum(Calendar.DAY_OF_WEEK)*/8 - start.get(Calendar.DAY_OF_WEEK)); if (start.getTimeInMillis() &gt; end.getTimeInMillis()) &#123; // 结束日期不能超过传来的最后日期 start.setTimeInMillis(end.getTimeInMillis()); &#125; String weekEnd = format.format(new Date(start.getTimeInMillis())); map.put(\"endTime\", weekEnd + \" 23:59:59\"); timeForQuery.add(map); timeForShow.add(startStr.substring(5, 10) + \"/\" + weekEnd.substring(5, 10)); // 加一天到下一周 start.add(Calendar.DAY_OF_WEEK, 1); &#125; &#125; /** * 按月分割 * * @param startTime * @param endTime * @param format * @param timeForQuery * @param timeForShow */ public static void separateByMonth(Date startTime, Date endTime, SimpleDateFormat format, List&lt;Map&lt;String, String&gt;&gt; timeForQuery, List&lt;String&gt; timeForShow) &#123; if (timeForQuery == null) &#123; timeForQuery = new ArrayList&lt;&gt;(); &#125; if (timeForShow == null) &#123; timeForShow = new ArrayList&lt;&gt;(); &#125; Calendar start = Calendar.getInstance(); start.setTime(startTime); Calendar end = Calendar.getInstance(); end.setTime(endTime); while (start.getTimeInMillis() &lt; end.getTimeInMillis()) &#123; HashMap&lt;String, String&gt; map = new HashMap&lt;&gt;(); String startStr = format.format(new Date(start.getTimeInMillis())); map.put(\"startTime\", startStr + \" 00:00:00\"); // region 按照自然月方式处理 start.set(Calendar.DAY_OF_MONTH, start.getActualMaximum(Calendar.DAY_OF_MONTH)); // endregion if (start.getTimeInMillis() &gt; end.getTimeInMillis()) &#123; start.setTimeInMillis(end.getTimeInMillis());// 不能超过end时期 &#125; Date monthEnd = new Date(start.getTimeInMillis()); map.put(\"endTime\", format.format(monthEnd) + \" 23:59:59\"); timeForQuery.add(map); timeForShow.add(format.format(monthEnd).substring(2, 7)); start.add(Calendar.DAY_OF_MONTH, 1); // 跳到下一天 &#125; &#125; /** * 按季度拆分 * * @param startTime * @param endTime * @param format * @param timeForQuery * @param timeForShow */ public static void separateByQuarter(Date startTime, Date endTime, SimpleDateFormat format, List&lt;Map&lt;String, String&gt;&gt; timeForQuery, List&lt;String&gt; timeForShow) &#123; if (timeForQuery == null) &#123; timeForQuery = new ArrayList&lt;&gt;(); &#125; if (timeForShow == null) &#123; timeForShow = new ArrayList&lt;&gt;(); &#125; Calendar start = Calendar.getInstance(); start.setTime(startTime); Calendar end = Calendar.getInstance(); end.setTime(endTime); List&lt;Integer&gt; quarter1 = Arrays.asList(0, 1, 2); List&lt;Integer&gt; quarter2 = Arrays.asList(3, 4, 5); List&lt;Integer&gt; quarter3 = Arrays.asList(6, 7, 8); List&lt;Integer&gt; quarter4 = Arrays.asList(9, 10, 11); while (start.getTimeInMillis() &lt; end.getTimeInMillis()) &#123; int curMonth = start.get(Calendar.MONTH); // 0-11 // 处理第一季度的月份 if (quarter1.contains(curMonth)) &#123; handleQuarterOnce(quarter1, 1, timeForQuery, timeForShow, format, start, end); &#125; // 处理第二季度的月份 if (quarter2.contains(curMonth)) &#123; handleQuarterOnce(quarter2, 2, timeForQuery, timeForShow, format, start, end); &#125; // 处理第三季度的月份 if (quarter3.contains(curMonth)) &#123; handleQuarterOnce(quarter3, 3, timeForQuery, timeForShow, format, start, end); &#125; // 处理第四季度的月份 if (quarter4.contains(curMonth)) &#123; handleQuarterOnce(quarter4, 4, timeForQuery, timeForShow, format, start, end); &#125; &#125; &#125; /** * 按年分隔 * * @param startTime * @param endTime * @param format * @param timeForQuery * @param timeForShow */ public static void separateByYear(Date startTime, Date endTime, SimpleDateFormat format, List&lt;Map&lt;String, String&gt;&gt; timeForQuery, List&lt;String&gt; timeForShow) &#123; if (timeForQuery == null) &#123; timeForQuery = new ArrayList&lt;&gt;(); &#125; if (timeForShow == null) &#123; timeForShow = new ArrayList&lt;&gt;(); &#125; Calendar start = Calendar.getInstance(); start.setTime(startTime); Calendar end = Calendar.getInstance(); end.setTime(endTime); while (start.getTimeInMillis() &lt; end.getTimeInMillis()) &#123; HashMap&lt;String, String&gt; map = new HashMap&lt;&gt;(); String startStr = format.format(new Date(start.getTimeInMillis())); map.put(\"startTime\", startStr + \" 00:00:00\"); // region按自然月处理 start.set(Calendar.DAY_OF_YEAR, start.getActualMaximum(Calendar.DAY_OF_YEAR)); // endregion if (start.getTimeInMillis() &gt; end.getTimeInMillis()) &#123; start.setTimeInMillis(end.getTimeInMillis()); &#125; String yearEnd = format.format(new Date(start.getTimeInMillis())); map.put(\"endTime\", yearEnd + \" 23:59:59\"); timeForQuery.add(map); timeForShow.add(yearEnd.substring(0, 4)); start.add(Calendar.DAY_OF_YEAR, 1);// 跳下一天 &#125; &#125; /**************************************************私有方法*********************************************************/ /** * 近供分割季度的方法使用 * * @param monthInQuarter List&lt;Integer&gt; quarter1 = Arrays.asList(0, 1, 2); * @param quarter 1\\2\\3\\4 * @param timeForQuery * @param timeForShow * @param format * @param start * @param end */ private static void handleQuarterOnce(List&lt;Integer&gt; monthInQuarter, int quarter, List&lt;Map&lt;String, String&gt;&gt; timeForQuery, List&lt;String&gt; timeForShow, SimpleDateFormat format, Calendar start, Calendar end) &#123; HashMap&lt;String, String&gt; map = new HashMap&lt;&gt;(); String startStr = format.format(start.getTimeInMillis()); map.put(\"startTime\", startStr + \" 00:00:00\"); // 设置为季度最后一天 // region 自然月的方式处理 start.set(Calendar.MONTH, monthInQuarter.get(2)); start.set(Calendar.DAY_OF_MONTH, start.getActualMaximum(Calendar.DAY_OF_MONTH)); // endregion // 防止超出时间范围 if (start.getTimeInMillis() &gt; end.getTimeInMillis()) &#123; start.setTimeInMillis(end.getTimeInMillis()); &#125; String quarterEnd = format.format(new Date(start.getTimeInMillis())); map.put(\"endTime\", quarterEnd + \" 23:59:59\"); timeForQuery.add(map); timeForShow.add(quarterEnd.substring(0, 4) + \"年\" + quarter + \"季度\"); start.add(Calendar.DAY_OF_MONTH, 1);// +1天 &#125; /*****************************************************测试*********************************************************/ public static void main(String[] args) throws Exception &#123; Calendar instance = Calendar.getInstance(); instance.add(Calendar.DAY_OF_YEAR, 300); SimpleDateFormat format = new SimpleDateFormat(\"yyyy-MM-dd\");// Date now = new Date();// Date end = new Date(instance.getTimeInMillis()); Date now = format.parse(\"2017-09-26\"); Date end = format.parse(\"2019-04-20\");// List&lt;Map&lt;String, String&gt;&gt; maps = separateByDay(new Date(), new Date(instance.getTimeInMillis()), format);// System.out.println(maps); ArrayList&lt;Map&lt;String, String&gt;&gt; timeForQuery = new ArrayList&lt;&gt;(); ArrayList&lt;String&gt; timeForShow = new ArrayList&lt;&gt;();// separateByWeek(now, end, format, timeForQuery, timeForShow);// separateByMonth(now, end, format, timeForQuery, timeForShow);// assert (!ObjectUtils.hasLength(timeForQuery)):\"OJBK\";// separateByQuarter(now, end, format, timeForQuery, timeForShow); separateByYear(now, end, format, timeForQuery, timeForShow); System.out.println(timeForQuery); System.out.println(timeForShow); &#125;&#125;","categories":[],"tags":[]}]}